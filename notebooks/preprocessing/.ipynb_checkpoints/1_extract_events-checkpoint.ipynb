{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.14.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "plotly.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Global variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines where files are located\n",
    "root = '/hpc/brainets/data/db_ebrains/seeg/Cohort %s/%s/%s/*_MCSE_*.pos'\n",
    "subject = 'HID-Sub-000'\n",
    "sfreq = 64.\n",
    "\n",
    "# load the cohort config file\n",
    "with open(\"../../seeg-ebrains/config/db_cohort.json\", 'r') as f:\n",
    "    cfg = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Global functions**\n",
    "## Read the triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pos(path_pos):\n",
    "    \"\"\"Path to the pos file and return a dataframe containing the triggers and samples\n",
    "    at which triggers appeared.\"\"\"\n",
    "    # read the file\n",
    "    r = np.genfromtxt(path_pos)\n",
    "    assert (r.ndim == 2) and (r.shape[1] == 3)\n",
    "    # dataframe conversion\n",
    "    df = pd.DataFrame(r[:, 0:2].astype(int), columns=['sample', 'trigger'])\n",
    "    # trigger remapping\n",
    "    remap = {11: 10, 12: 10, 61: 60, 62: 60, 1: 100, 2: 200}\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_events(df):\n",
    "    \"\"\"Detect events.\n",
    "    \n",
    "    This function takes as input the dataframe containing triggers and samples\n",
    "    and return a dataframe which all of the detected events (easy /hard blocks;\n",
    "    reponse; response time; correct / incorrect)\n",
    "    \"\"\"\n",
    "    # detect consecutive events\n",
    "    block, response, t_block = [], [], None\n",
    "    for n_t, t in enumerate(list(df['trigger'])):\n",
    "        if t in [11, 12, 61, 62]:\n",
    "            t_block = n_t\n",
    "        elif t in [1, 2] and t_block is not None:\n",
    "            block += [t_block]\n",
    "            response += [n_t]\n",
    "            t_block = None\n",
    "    # build dataframe\n",
    "    dat = np.c_[df.loc[block, 'sample'], df.loc[response, 'sample'],\n",
    "                df.loc[block, 'trigger'], df.loc[response, 'trigger']].astype(int)\n",
    "    df_t = pd.DataFrame(dat, columns=['sample_stim', 'sample_resp', 'trig_stim', 'trig_resp'])\n",
    "    # insert timings\n",
    "    df_t['time_stim'] = df_t['sample_stim'] / sfreq\n",
    "    df_t['time_resp'] = df_t['sample_resp'] / sfreq\n",
    "    df_t['response_time'] = df_t['time_resp'] - df_t['time_stim']\n",
    "    # add if correct answer\n",
    "    df_t['correct'] = np.mod(dat[:, -2] - dat[:, -1], 10) == 0\n",
    "    # add column for difficulty of the task\n",
    "    df_t['difficulty'] = ['easy'] * len(df_t)\n",
    "    stim = np.array(list(df_t['trig_stim']))\n",
    "    df_t.loc[np.logical_or(stim == 61, stim == 62), 'difficulty'] = 'hard'\n",
    "    # add orientation of the \"T\" letter\n",
    "    df_t['T_orientation'] = ['T down'] * len(df_t)\n",
    "    df_t.loc[np.logical_or(stim == 12, stim == 62), 'T_orientation'] = 'T up'\n",
    "    return df_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Detect and save the events**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -----------SUBJECT 1 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 2 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 3 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 4 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 5 ALREADY EXTRACTED. SKIPPED -----------\n",
      "SUBJECT 6 FAILED (not enough trials. SKIPPED)\n",
      "SUBJECT 7 FAILED (not enough trials. SKIPPED)\n",
      " -----------SUBJECT 8 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 9 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 10 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 11 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 12 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 13 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 14 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 15 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 16 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 17 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 18 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 19 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 20 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 21 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 22 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 23 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 24 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 25 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 26 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 27 ALREADY EXTRACTED. SKIPPED -----------\n",
      "Subject 28 has no file. SKIPPED\n",
      " -----------SUBJECT 29 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 30 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 31 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 32 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 33 ALREADY EXTRACTED. SKIPPED -----------\n",
      "SUBJECT 34 FAILED (not enough trials. SKIPPED)\n",
      " -----------SUBJECT 35 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 36 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 37 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 38 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 39 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 40 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 41 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 42 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 43 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 44 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 45 ALREADY EXTRACTED. SKIPPED -----------\n",
      "Subject 46 has no file. SKIPPED\n",
      " -----------SUBJECT 47 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 48 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 49 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 50 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 51 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 52 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 53 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 54 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 55 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 56 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 57 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 58 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 59 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 60 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 61 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 62 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 63 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 64 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 65 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 66 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 67 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 68 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 69 ALREADY EXTRACTED. SKIPPED -----------\n",
      " -----------SUBJECT 70 ALREADY EXTRACTED. SKIPPED -----------\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "save_to = '/hpc/brainets/data/db_ebrains/analysis/events'\n",
    "##########################################################################\n",
    "\n",
    "if not os.path.isdir(save_to): os.mkdir(save_to)\n",
    "    \n",
    "for coh in ['I', 'II', 'III']:\n",
    "    suj_range = cfg[coh]['s_range']\n",
    "    seeg_folder = cfg[coh]['seeg_folder']\n",
    "    for suj in range(suj_range[0], suj_range[1] + 1):\n",
    "        print(f\"Cohort {coh} : {suj}\", end='\\r')\n",
    "        # define subject name\n",
    "        id_suj = str(suj)\n",
    "        s_name = subject[0:-len(id_suj)] + id_suj\n",
    "        save_as = os.path.join(save_to, f\"{s_name}.xlsx\")\n",
    "        \n",
    "        # if already saved, skip\n",
    "        if os.path.isfile(save_as):\n",
    "            print(f\" -----------SUBJECT {suj} ALREADY EXTRACTED. SKIPPED -----------\")\n",
    "            continue\n",
    "        \n",
    "        # define path to the pos file\n",
    "        path_pos = root % (coh, seeg_folder, s_name)\n",
    "        \n",
    "        # get pos file name\n",
    "        file = glob.glob(path_pos)\n",
    "        if len(file) != 1:\n",
    "            print(f\"Subject {suj} has no file. SKIPPED\")\n",
    "            continue\n",
    "        \n",
    "        # read the triggers and events\n",
    "        df_trig = read_pos(file[0])\n",
    "        df_ev = detect_events(df_trig)\n",
    "        \n",
    "        # track those fow which events detection failed\n",
    "        if len(df_ev) < 10:\n",
    "            # here we say that if there's not at least, 10 events, it's considered\n",
    "            # as a bad file\n",
    "            print(f\"SUBJECT {suj} FAILED (not enough trials. SKIPPED)\")\n",
    "            continue\n",
    "\n",
    "        # save the events\n",
    "        \n",
    "        df_ev.to_excel(save_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
